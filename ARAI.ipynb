{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "import uuid\n",
    "from arai.utils.utils import Document\n",
    "from arai.utils.llm_utils import llm_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_wrapper(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './IS_example/STORM.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./IS_example/STORM.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2[==                                      ] ( 2/27=[====                                    ] ( 3/27[=====                                   ] ( 4/2=[=======                                 ] ( 5/2[========                                ] ( 6/27=[==========                              ] ( 7/27[===========                             ] ( 8/2=[=============                           ] ( 9/2[==============                          ] (10/27=[================                        ] (11/27[=================                       ] (12/2=[===================                     ] (13/2[====================                    ] (14/27=[======================                  ] (15/27[=======================                 ] (16/2=[=========================               ] (17/2[==========================              ] (18/27=[============================            ] (19/27[=============================           ] (20/2=[===============================         ] (21/2[================================        ] (22/27=[==================================      ] (23/27[===================================     ] (24/2=[=====================================   ] (25/2[======================================  ] (26/27[========================================] (27/27]\n"
     ]
    }
   ],
   "source": [
    "pages = pymupdf4llm.to_markdown(path, page_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        id=str(uuid.uuid4()), \n",
    "        page_content=page['text'], \n",
    "        metadata=dict(\n",
    "            page=page['metadata']['page'],\n",
    "            source=page['metadata']['file_path']\n",
    "        ),\n",
    "        type=\"document\"\n",
    "    ) \n",
    "    for page in pages\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_1 = documents[0]\n",
    "\n",
    "background = llm_wrapper(prompt=f\"{page_1.page_content}\\n\\nRead the text above and summarize it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arai.agents.online.inputagent import InputAgent\n",
    "from arai.agents.online.organizer import OrganizerAgent\n",
    "from arai.agents.online.jargon_detector import JargonDetector\n",
    "from arai.memories.events import EventBus\n",
    "\n",
    "event_bus = EventBus()\n",
    "input_agent = InputAgent()\n",
    "organizer_agent = OrganizerAgent()\n",
    "jargon_translator = JargonDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputAgent: Processing input: S.O.S. is an abbreviation for Save Our Souls.\n",
      "OrganizerAgent: Routing request...\n",
      "OrganizerAgent: parsing\n",
      "JargonDetector: Extracting jargons, abbreviations, acronyms from INPUT...\n",
      "JargonDetector: parsing\n"
     ]
    }
   ],
   "source": [
    "input_agent.start_conversation(user_input=\"S.O.S. is an abbreviation for Save Our Souls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'S.O.S. is an abbreviation for Save Our Souls.',\n",
       " 'jargons': {'jargons': [{'jargon': 'S.O.S.'}]},\n",
       " 'source': 'JargonDetector',\n",
       " 'timestamp': 1734835593,\n",
       " 'event_type': 'editing_needed'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_bus.get_latest_event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'S.O.S. is an abbreviation for Save Our Souls.',\n",
       "  'source': 'InputAgent',\n",
       "  'timestamp': 1734835592,\n",
       "  'event_type': 'input_received'},\n",
       " {'user_input': 'S.O.S. is an abbreviation for Save Our Souls.',\n",
       "  'reason': 'INPUT contains jargons, abbreviations, or acronyms',\n",
       "  'source': 'OrganizerAgent',\n",
       "  'timestamp': 1734835592,\n",
       "  'event_type': 'jargon_needed'},\n",
       " {'user_input': 'S.O.S. is an abbreviation for Save Our Souls.',\n",
       "  'jargons': {'jargons': [{'jargon': 'S.O.S.'}]},\n",
       "  'source': 'JargonDetector',\n",
       "  'timestamp': 1734835593,\n",
       "  'event_type': 'editing_needed'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_bus.get_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arai.memories.base import MemoryManagement\n",
    "mm = MemoryManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database, schema, name, column_names, column_types, temporary]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.list_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.drop_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database, schema, name, column_names, column_types, temporary]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.list_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.create_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memory</td>\n",
       "      <td>main</td>\n",
       "      <td>mockmemory</td>\n",
       "      <td>[id, page_content, metadata, embedding, create...</td>\n",
       "      <td>[VARCHAR, VARCHAR, VARCHAR, FLOAT[100], INTEGE...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database schema        name  \\\n",
       "0   memory   main  mockmemory   \n",
       "\n",
       "                                        column_names  \\\n",
       "0  [id, page_content, metadata, embedding, create...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0  [VARCHAR, VARCHAR, VARCHAR, FLOAT[100], INTEGE...      False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.list_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arai.memories.long_term import LongTermMemory\n",
    "\n",
    "ltm = LongTermMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memory</td>\n",
       "      <td>main</td>\n",
       "      <td>longtermschemas</td>\n",
       "      <td>[id, page_content, metadata, embedding, create...</td>\n",
       "      <td>[VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database schema             name  \\\n",
       "0   memory   main  longtermschemas   \n",
       "\n",
       "                                        column_names  \\\n",
       "0  [id, page_content, metadata, embedding, create...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0  [VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...      False  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.list_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memory</td>\n",
       "      <td>main</td>\n",
       "      <td>longtermschemas</td>\n",
       "      <td>[id, page_content, metadata, embedding, create...</td>\n",
       "      <td>[VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database schema             name  \\\n",
       "0   memory   main  longtermschemas   \n",
       "\n",
       "                                        column_names  \\\n",
       "0  [id, page_content, metadata, embedding, create...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0  [VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...      False  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.list_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm.add_document(\n",
    "    document=[\"1\", \"page_content\", \"metadata\", [1.1,1.2], 12345, 12345]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm.create_fts_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>schema</th>\n",
       "      <th>name</th>\n",
       "      <th>column_names</th>\n",
       "      <th>column_types</th>\n",
       "      <th>temporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>dict</td>\n",
       "      <td>[termid, term, df]</td>\n",
       "      <td>[BIGINT, VARCHAR, BIGINT]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>docs</td>\n",
       "      <td>[docid, name, len]</td>\n",
       "      <td>[BIGINT, VARCHAR, BIGINT]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>fields</td>\n",
       "      <td>[fieldid, field]</td>\n",
       "      <td>[BIGINT, VARCHAR]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>stats</td>\n",
       "      <td>[num_docs, avgdl]</td>\n",
       "      <td>[BIGINT, DOUBLE]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>stopwords</td>\n",
       "      <td>[sw]</td>\n",
       "      <td>[VARCHAR]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>memory</td>\n",
       "      <td>fts_main_longtermschemas</td>\n",
       "      <td>terms</td>\n",
       "      <td>[docid, fieldid, termid]</td>\n",
       "      <td>[BIGINT, BIGINT, BIGINT]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>memory</td>\n",
       "      <td>main</td>\n",
       "      <td>longtermschemas</td>\n",
       "      <td>[id, page_content, metadata, embedding, create...</td>\n",
       "      <td>[VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  database                    schema             name  \\\n",
       "0   memory  fts_main_longtermschemas             dict   \n",
       "1   memory  fts_main_longtermschemas             docs   \n",
       "2   memory  fts_main_longtermschemas           fields   \n",
       "3   memory  fts_main_longtermschemas            stats   \n",
       "4   memory  fts_main_longtermschemas        stopwords   \n",
       "5   memory  fts_main_longtermschemas            terms   \n",
       "6   memory                      main  longtermschemas   \n",
       "\n",
       "                                        column_names  \\\n",
       "0                                 [termid, term, df]   \n",
       "1                                 [docid, name, len]   \n",
       "2                                   [fieldid, field]   \n",
       "3                                  [num_docs, avgdl]   \n",
       "4                                               [sw]   \n",
       "5                           [docid, fieldid, termid]   \n",
       "6  [id, page_content, metadata, embedding, create...   \n",
       "\n",
       "                                        column_types  temporary  \n",
       "0                          [BIGINT, VARCHAR, BIGINT]      False  \n",
       "1                          [BIGINT, VARCHAR, BIGINT]      False  \n",
       "2                                  [BIGINT, VARCHAR]      False  \n",
       "3                                   [BIGINT, DOUBLE]      False  \n",
       "4                                          [VARCHAR]      False  \n",
       "5                           [BIGINT, BIGINT, BIGINT]      False  \n",
       "6  [VARCHAR, VARCHAR, VARCHAR, FLOAT[2], INTEGER,...      False  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.list_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>embedding</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>page_content</td>\n",
       "      <td>metadata</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>12345</td>\n",
       "      <td>12345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  page_content  metadata   embedding  created_at  updated_at\n",
       "0  1  page_content  metadata  [1.1, 1.2]       12345       12345"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>page_content</td>\n",
       "      <td>metadata</td>\n",
       "      <td>0.124939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  page_content  metadata     score\n",
       "0  1  page_content  metadata  0.124939"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.fts_search(query=\"page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>embedding</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>page_content</td>\n",
       "      <td>metadata</td>\n",
       "      <td>[1.1, 1.2]</td>\n",
       "      <td>12345</td>\n",
       "      <td>12345</td>\n",
       "      <td>0.994044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  page_content  metadata   embedding  created_at  updated_at     score\n",
       "0  1  page_content  metadata  [1.1, 1.2]       12345       12345  0.994044"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.vector_search(query=[1.1,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm.drop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    [1,2,3,4,5,6,7,8,9,10],\n",
    "    [11,12,13,14,15,16,17,18,19,20],\n",
    "    [21,22,23,24,25,26,27,28,29,30],\n",
    "    [31,32,33]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = [i for i in range(40)]\n",
    "document2 = [i for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents, chunk_size=10, overlap=1):\n",
    "    max = len(documents)\n",
    "    start_idx = 0\n",
    "    end_idx = chunk_size\n",
    "    indexes = [(start_idx, end_idx)]\n",
    "    response = []\n",
    "    while end_idx < max:        \n",
    "        start_idx = end_idx - overlap\n",
    "        end_idx = start_idx + chunk_size\n",
    "        indexes.append((start_idx, end_idx))\n",
    "        response.append(documents[start_idx:end_idx])\n",
    "    return max, indexes, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = documents[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7f2145d3-16e5-4788-a21a-d14937750006',\n",
       " 'page_content': '## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\\n\\n### Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\\n\\n\\n### Abstract\\n\\n\\nWe study how to apply large language models\\nto write grounded and organized long-form articles from scratch, with comparable breadth\\nand depth to Wikipedia pages. This underexplored problem poses new challenges at the\\n_pre-writing stage, including how to research_\\nthe topic and prepare an outline prior to writing. We propose STORM, a writing system\\nfor the Synthesis of Topic Outlines through\\n**Retrieval and Multi-perspective Question Ask-**\\ning. STORM models the pre-writing stage by\\n(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded\\non trusted Internet sources, (3) curating the collected information to create an outline.\\n\\nFor evaluation, we curate FreshWiki, a dataset\\nof recent high-quality Wikipedia articles, and\\nformulate outline assessments to evaluate the\\npre-writing stage. We further gather feedback\\nfrom experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of\\nSTORM’s articles are deemed to be organized\\n(by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also\\nhelps identify new challenges for generating\\ngrounded long articles, such as source bias\\ntransfer and over-association of unrelated facts.\\n\\n### 1 Introduction\\n\\n\\n**LLM-** Winter Olympics opening ceremony determined?\\n**Role1**\\n\\nFigure 1: We explore writing Wikipedia-like articles\\nfrom scratch, which demands a pre-writing stage before\\nproducing the article. In this stage, simpler approaches\\nlike Direct Prompting have limited planning capacity. In\\ncontrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\\n\\n\\nLarge language models (LLMs) have demonstrated\\nimpressive writing capabilities (Yang et al., 2023;\\nPavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria,\\n2023), but it is unclear how we can use them to\\nwrite grounded, long-form articles, like full-length\\nWikipedia pages. Such expository writing, which\\nseeks to inform the reader on a topic in an organized manner (Weaver III and Kintsch, 1991;\\nBalepur et al., 2023), requires thorough research\\nand planning in the pre-writing stage (Rohman,\\n\\n\\n1965), even before the actual writing process can\\nstart. However, prior work on generating Wikipedia\\narticles (Banerjee and Mitra, 2015; Minguillón\\net al., 2017; Liu et al., 2018; Fan and Gardent,\\n2022) has generally bypassed the pre-writing stage:\\nfor instance, Liu et al. (2018) presume reference\\ndocuments are provided in advance, while Fan and\\nGardent (2022) assume an article outline is available and focus on expanding each section. These\\nassumptions do not hold in general, as collecting\\nreferences and crafting outlines demand advanced\\ninformation literacy skills (Doyle, 1994) to iden\\n\\n-----\\n\\n',\n",
       " 'metadata': {'page': 1, 'source': './IS_example/STORM.pdf'},\n",
       " 'type': 'document'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "chunk_size = 100\n",
    "overlap = int(chunk_size * 0.3)\n",
    "\n",
    "# def validate_chunk_size(chunks:List[str], chunk_size:int)->int | None:\n",
    "#     \"\"\"validate chunk size and overlap\n",
    "    \n",
    "#     Steps:\n",
    "#         - check if: chunks > 0\n",
    "#         - create gap = chunk_size - chunks[-1]\n",
    "#         - check if: gap > 0\n",
    "#         - return gap\n",
    "#     \"\"\"\n",
    "#     if len(chunks) > 0:\n",
    "#         gap = chunk_size - len(chunks[-1].split(\" \"))\n",
    "#         if gap > 0:\n",
    "#             return gap\n",
    "#     return None\n",
    "\n",
    "# def chunk_documents(documents:List[Document], chunk_size:int=100, overlap:int=30)->List[Document]:\n",
    "#     chunks:List[str] = []\n",
    "#     for doc in documents:\n",
    "#         contents = doc.page_content.replace(\"-\\n\",\"\").replace(\"\\n\\n\", \"\\n\").split(\" \")\n",
    "#         gap = validate_chunk_size(chunks, chunk_size)\n",
    "#         if gap is not None:\n",
    "#             print(gap)\n",
    "#             last_chunk = chunks[-1].split(\" \")\n",
    "#             chunks[-1] = \" \".join(last_chunk + contents[:gap])\n",
    "#         max_length = len(contents)\n",
    "#         start_idx = 0\n",
    "#         end_idx = chunk_size\n",
    "#         response = []\n",
    "#         while True:\n",
    "#             response.append(\" \".join(contents[start_idx:end_idx]))\n",
    "#             if end_idx > max_length:\n",
    "#                 break\n",
    "#             start_idx = end_idx - overlap\n",
    "#             end_idx = start_idx + chunk_size\n",
    "#         chunks.extend(response)\n",
    "#     return chunks\n",
    "\n",
    "def validate_chunk_size(chunks:List[str], chunk_size:int)->bool:\n",
    "    if len(chunks) > 0:\n",
    "        gap = chunk_size - len(chunks[-1].split(\" \"))\n",
    "        if gap > 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def chunk_normal(documents:List[Document], chunks:List[str], chunk_size:int, overlap:int, cnt:int)->Tuple[List[str], int]:\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "        - page = documents[cnt]\n",
    "        - page = page.page_content.split(\" \")\n",
    "        - max_length = len(page)\n",
    "        - start_idx, end_idx = 0, chunk_size\n",
    "        - loop\n",
    "            - chunk = \" \".join(page[start_idx:end_idx])\n",
    "            - chunks.append(chunk)\n",
    "            - if end_idx > max_length\n",
    "                - break\n",
    "            - start_idx = end_idx - overlap\n",
    "            - end_idx = start_idx + chunk_size\n",
    "    \"\"\"\n",
    "    # print(\"chunk_normal\")\n",
    "    page = documents[cnt]\n",
    "    page = page.page_content.split(\" \")\n",
    "    max_length = len(page)\n",
    "    start_idx = 0\n",
    "    end_idx = chunk_size\n",
    "    while True:\n",
    "        chunks.append(\" \".join(page[start_idx:end_idx]))\n",
    "        if end_idx > max_length:\n",
    "            break\n",
    "        start_idx = end_idx - overlap\n",
    "        end_idx = start_idx + chunk_size\n",
    "    return chunks, cnt+1\n",
    "\n",
    "def chunk_forward(documents:List[Document], chunks:List[str], chunk_size:int, threshold:int, cnt:int)->Tuple[str, int, int]:\n",
    "    \"\"\"repeat filling the chunk until it meets the chunk_size or threshold\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): documents parsed by pymupdf as pages\n",
    "        chunks (List[str]): chunks of documents\n",
    "        chunk_size (int): how many words in a chunk\n",
    "        threshold (int): maximum number of documents\n",
    "        cnt (int): document index\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, int, int]: chunk will replace chunks[-1], dy_cnt is a new document index, offset is the index of a new document\n",
    "    \"\"\"\n",
    "    # print(\"chunk_forward\")\n",
    "    dy_cnt = cnt\n",
    "    last_chunk = chunks[-1].split(\" \")\n",
    "    while dy_cnt < threshold:\n",
    "        gap = chunk_size - len(last_chunk)\n",
    "        if gap == 0:\n",
    "            break\n",
    "        offset = gap\n",
    "        page = documents[dy_cnt]\n",
    "        page = page.page_content.split(\" \")\n",
    "        fill = page[:offset]\n",
    "        last_chunk = last_chunk + fill\n",
    "        dy_cnt += 1\n",
    "    chunk = \" \".join(last_chunk)\n",
    "    return chunk, dy_cnt, offset\n",
    "\n",
    "def chunk_backward(documents:List[Document], chunk_size:int, overlap:int, offset:int, cnt:int)->Tuple[str, int, int]:\n",
    "    dy_cnt = cnt-1\n",
    "    end_idx = (chunk_size-overlap) + offset\n",
    "    chunk = documents[cnt].page_content.split(\" \")[:end_idx]\n",
    "    while dy_cnt > 0:\n",
    "        gap = chunk_size - len(chunk)\n",
    "        if gap == 0:\n",
    "            break\n",
    "        page = documents[dy_cnt]\n",
    "        fill = page.page_content.split(\" \")[-gap:]\n",
    "        chunk = fill + chunk\n",
    "        dy_cnt -= 1\n",
    "    chunk = \" \".join(chunk)\n",
    "    return chunk, dy_cnt, 0\n",
    "\n",
    "def validate_offset(offset:int, overlap:int)->bool:\n",
    "    if offset!=0:\n",
    "        if (offset - overlap) < 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def chunk_documents(documents:List[Document], chunk_size:int=100, overlap:int=30)->List[Document]:\n",
    "    threshold = len(documents)\n",
    "    cnt = 0\n",
    "    offset = 0\n",
    "    chunks:List[str] = []\n",
    "    while cnt < threshold:\n",
    "        if validate_chunk_size(chunks, chunk_size):\n",
    "            chunk, _, offset = chunk_forward(documents, chunks, chunk_size, threshold, cnt)\n",
    "            chunks[-1] = chunk\n",
    "        # if validate_offset(offset, overlap): # this is not working\n",
    "        #     chunk, cnt, offset = chunk_backward(documents, chunk_size, overlap, offset, cnt)\n",
    "        #     chunks.append(chunk)\n",
    "        if cnt >= threshold:\n",
    "            break\n",
    "        chunks, cnt = chunk_normal(documents, chunks, chunk_size, overlap, cnt)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arai.utils.chunking import CustomChunker\n",
    "\n",
    "chunk_size = 700\n",
    "overlap = int(chunk_size*0.3)\n",
    "chunker = CustomChunker(chunk_size=chunk_size, overlap=overlap)\n",
    "_ = chunker.run(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 with size 12423\n"
     ]
    }
   ],
   "source": [
    "_ = chunker.chunks\n",
    "chunker.check_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Assisting in Writing Wikipedia-like Articles From Scratch with Large\n"
     ]
    }
   ],
   "source": [
    "print(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch with Large\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(_[0].split(\" \")[-overlap:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch with Large\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(_[1].split(\" \")[:overlap]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch with Large Language Models\n",
      "\n",
      "### Yijia Shao Yucheng Jiang Theodore\n"
     ]
    }
   ],
   "source": [
    "print(_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n",
      "\n",
      "### Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\n",
      "\n",
      "\n",
      "### Abstract\n",
      "\n",
      "\n",
      "We study how to apply large language models\n",
      "to write grounded and organized long-form articles from scratch, with comparable breadth\n",
      "and depth to Wikipedia pages. This underexplored problem poses new challenges at the\n",
      "_pre-writing stage, including how to research_\n",
      "the topic and prepare an outline prior to writing. We propose STORM, a writing system\n",
      "for the Synthesis of Topic Outlines through\n",
      "**Retrieval and Multi-perspective Question Ask-**\n",
      "ing. STORM models the pre-writing stage by\n",
      "(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded\n",
      "on trusted Internet sources, (3) curating the collected information to create an outline.\n",
      "\n",
      "For evaluation, we curate FreshWiki, a dataset\n",
      "of recent high-quality Wikipedia articles, and\n",
      "formulate outline assessments to evaluate the\n",
      "pre-writing stage. We further gather feedback\n",
      "from experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of\n",
      "STORM’s articles are deemed to be organized\n",
      "(by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also\n",
      "helps identify new challenges for generating\n",
      "grounded long articles, such as source bias\n",
      "transfer and over-association of unrelated facts.\n",
      "\n",
      "### 1 Introduction\n",
      "\n",
      "\n",
      "**LLM-** Winter Olympics opening ceremony determined?\n",
      "**Role1**\n",
      "\n",
      "Figure 1: We explore writing Wikipedia-like articles\n",
      "from scratch, which demands a pre-writing stage before\n",
      "producing the article. In this stage, simpler approaches\n",
      "like Direct Prompting have limited planning capacity. In\n",
      "contrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\n",
      "\n",
      "\n",
      "Large language models (LLMs) have demonstrated\n",
      "impressive writing capabilities (Yang et al., 2023;\n",
      "Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria,\n",
      "2023), but it is unclear how we can use them to\n",
      "write grounded, long-form articles, like full-length\n",
      "Wikipedia pages. Such expository writing, which\n",
      "seeks to inform the reader on a topic in an organized manner (Weaver\n"
     ]
    }
   ],
   "source": [
    "print(_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n",
      "\n",
      "### Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\n",
      "\n",
      "\n",
      "### Abstract\n",
      "\n",
      "\n",
      "We study how to apply large language models\n",
      "to write grounded and organized long-form articles from scratch, with comparable breadth\n",
      "and depth to Wikipedia pages. This underexplored problem poses new challenges at the\n",
      "_pre-writing stage, including how to research_\n",
      "the topic and prepare an outline prior to writing. We propose STORM, a writing system\n",
      "for the Synthesis of Topic Outlines through\n",
      "**Retrieval and Multi-perspective Question Ask-**\n",
      "ing. STORM models the pre-writing stage by\n",
      "(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded\n",
      "on trusted Internet sources, (3) curating the collected information to create an outline.\n",
      "\n",
      "For evaluation, we curate FreshWiki, a dataset\n",
      "of recent high-quality Wikipedia articles, and\n",
      "formulate outline assessments to evaluate the\n",
      "pre-writing stage. We further gather feedback\n",
      "from experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of\n",
      "STORM’s articles are deemed to be organized\n",
      "(by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also\n",
      "helps identify new challenges for generating\n",
      "grounded long articles, such as source bias\n",
      "transfer and over-association of unrelated facts.\n",
      "\n",
      "### 1 Introduction\n",
      "\n",
      "\n",
      "**LLM-** Winter Olympics opening ceremony determined?\n",
      "**Role1**\n",
      "\n",
      "Figure 1: We explore writing Wikipedia-like articles\n",
      "from scratch, which demands a pre-writing stage before\n",
      "producing the article. In this stage, simpler approaches\n",
      "like Direct Prompting have limited planning capacity. In\n",
      "contrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\n",
      "\n",
      "\n",
      "Large language models (LLMs) have demonstrated\n",
      "impressive writing capabilities (Yang et al., 2023;\n",
      "Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria,\n",
      "2023), but it is unclear how we can use them to\n",
      "write grounded, long-form articles, like full-length\n",
      "Wikipedia pages. Such expository writing, which\n",
      "seeks to inform the reader on a topic in an organized manner (Weaver\n"
     ]
    }
   ],
   "source": [
    "print(_[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\n",
      "\n",
      "### Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam Stanford University {shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\n",
      "\n",
      "\n",
      "### Abstract\n",
      "\n",
      "\n",
      "We study how to apply large language models\n",
      "to write grounded and organized long-form articles from scratch, with comparable breadth\n",
      "and depth to Wikipedia pages. This underexplored problem poses new challenges at the\n",
      "_pre-writing stage, including how to research_\n",
      "the topic and prepare an outline prior to writing. We propose STORM, a writing system\n",
      "for the Synthesis of Topic Outlines through\n",
      "**Retrieval and Multi-perspective Question Ask-**\n",
      "ing. STORM models the pre-writing stage by\n",
      "(1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded\n",
      "on trusted Internet sources, (3) curating the collected information to create an outline.\n",
      "\n",
      "For evaluation, we curate FreshWiki, a dataset\n",
      "of recent high-quality Wikipedia articles, and\n",
      "formulate outline assessments to evaluate the\n",
      "pre-writing stage. We further gather feedback\n",
      "from experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of\n",
      "STORM’s articles are deemed to be organized\n",
      "(by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also\n",
      "helps identify new challenges for generating\n",
      "grounded long articles, such as source bias\n",
      "transfer and over-association of unrelated facts.\n",
      "\n",
      "### 1 Introduction\n",
      "\n",
      "\n",
      "**LLM-** Winter Olympics opening ceremony determined?\n",
      "**Role1**\n",
      "\n",
      "Figure 1: We explore writing Wikipedia-like articles\n",
      "from scratch, which demands a pre-writing stage before\n",
      "producing the article. In this stage, simpler approaches\n",
      "like Direct Prompting have limited planning capacity. In\n",
      "contrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\n",
      "\n",
      "\n",
      "Large language models (LLMs) have demonstrated\n",
      "impressive writing capabilities (Yang et al., 2023;\n",
      "Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria,\n",
      "2023), but it is unclear how we can use them to\n",
      "write grounded, long-form articles, like full-length\n",
      "Wikipedia pages. Such expository writing, which\n",
      "seeks to inform the reader on a topic in an organized manner (Weaver\n"
     ]
    }
   ],
   "source": [
    "print(_[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
